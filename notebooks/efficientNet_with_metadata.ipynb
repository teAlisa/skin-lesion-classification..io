{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b193931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\talic\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import os\n",
    "\n",
    "tf.keras.backend.clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ecfebfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = 'cleaned_data/ISIC_2019_Training_Input_cleaned'\n",
    "IMG_SIZE = (224, 224)\n",
    "data = pd.read_csv(\"cleaned_data\\\\combined_GT_MD.csv\")\n",
    "class_cols = ['MEL', 'NV', 'BCC', 'BKL', 'other']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adbe871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data[class_cols].idxmax(axis=1)\n",
    "label_map = {name: idx for idx, name in enumerate(class_cols)}\n",
    "data['label'] = data['label'].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4ba724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata columns\n",
    "meta_cols = [c for c in data.columns if c not in ['image', 'label', 'split', 'lesion_id'] + class_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f6b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_ids, folder):\n",
    "    images = []\n",
    "    for image_id in image_ids:\n",
    "        path = os.path.join(folder, image_id + '.jpg')\n",
    "        img = cv2.imread(path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: image {path} not found or can't be read.\")\n",
    "            continue\n",
    "        img = cv2.resize(img, IMG_SIZE)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        images.append(img)\n",
    "    return np.array(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c69babff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "train_df = data[data['split'] == 'train']\n",
    "val_df = data[data['split'] == 'val']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ee0f01",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_img \u001b[38;5;241m=\u001b[39m \u001b[43mload_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m X_val_img \u001b[38;5;241m=\u001b[39m load_images(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m], IMG_DIR)\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mload_images\u001b[1;34m(image_ids, folder)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_id \u001b[38;5;129;01min\u001b[39;00m image_ids:\n\u001b[0;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, image_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found or can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_img = load_images(train_df['image'], IMG_DIR)\n",
    "X_val_img = load_images(val_df['image'], IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0df03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_meta = train_df[meta_cols].values\n",
    "X_val_meta = val_df[meta_cols].values\n",
    "X_train_meta = X_train_meta.astype('float32')\n",
    "X_val_meta = X_val_meta.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4227a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(train_df['label'], num_classes=5)\n",
    "y_val = to_categorical(val_df['label'], num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2653a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (16284, 224, 224, 3)\n",
      "Train metadata shape: (16284, 13)\n",
      "Train labels shape: (16284, 5)\n",
      "Val images shape: (3588, 224, 224, 3)\n",
      "Val metadata shape: (3588, 13)\n",
      "Val labels shape: (3588, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train images shape: {X_train_img.shape}\")\n",
    "print(f\"Train metadata shape: {X_train_meta.shape}\")\n",
    "print(f\"Train labels shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"Val images shape: {X_val_img.shape}\")\n",
    "print(f\"Val metadata shape: {X_val_meta.shape}\")\n",
    "print(f\"Val labels shape: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cc326",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_input = Input(shape=(224, 224, 3), name='img_input')\n",
    "meta_input = Input(shape=(13,), name='meta_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the EfficientNetB0 model without the top layer\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=img_input)\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer for regularization\n",
    "x = Dropout(0.3)(x)\n",
    "# Concatenate image features with metadata\n",
    "x = Concatenate()([x, meta_input])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "# First dense layer with L2 regularization\n",
    "x = Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)              \n",
    "x = Dropout(0.4)(x)                     \n",
    "\n",
    "# Second dense layer with L2 regularization\n",
    "x = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer with softmax activation\n",
    "output = Dense(5, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c318d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Model(inputs=[img_input, meta_input], outputs=output)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "checkpoint_path = 'cnn_before.keras'\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "callbacks.append(reduce_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30a51e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1173s\u001b[0m 2s/step - accuracy: 0.2901 - loss: 2.3188 - val_accuracy: 0.4013 - val_loss: 1.8257 - learning_rate: 1.0000e-05\n",
      "Epoch 2/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1191s\u001b[0m 2s/step - accuracy: 0.3492 - loss: 2.1114 - val_accuracy: 0.3788 - val_loss: 1.8198 - learning_rate: 1.0000e-05\n",
      "Epoch 3/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1181s\u001b[0m 2s/step - accuracy: 0.4036 - loss: 1.9688 - val_accuracy: 0.4699 - val_loss: 1.6707 - learning_rate: 1.0000e-05\n",
      "Epoch 4/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1173s\u001b[0m 2s/step - accuracy: 0.4289 - loss: 1.8767 - val_accuracy: 0.4967 - val_loss: 1.6070 - learning_rate: 1.0000e-05\n",
      "Epoch 5/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1165s\u001b[0m 2s/step - accuracy: 0.4457 - loss: 1.8319 - val_accuracy: 0.4189 - val_loss: 1.7332 - learning_rate: 1.0000e-05\n",
      "Epoch 6/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1185s\u001b[0m 2s/step - accuracy: 0.4758 - loss: 1.7346 - val_accuracy: 0.5220 - val_loss: 1.5701 - learning_rate: 1.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1175s\u001b[0m 2s/step - accuracy: 0.5002 - loss: 1.6958 - val_accuracy: 0.5231 - val_loss: 1.5546 - learning_rate: 1.0000e-05\n",
      "Epoch 8/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1177s\u001b[0m 2s/step - accuracy: 0.5181 - loss: 1.6593 - val_accuracy: 0.4877 - val_loss: 1.6335 - learning_rate: 1.0000e-05\n",
      "Epoch 9/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1176s\u001b[0m 2s/step - accuracy: 0.5333 - loss: 1.6125 - val_accuracy: 0.5875 - val_loss: 1.4833 - learning_rate: 1.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1183s\u001b[0m 2s/step - accuracy: 0.5367 - loss: 1.5809 - val_accuracy: 0.5688 - val_loss: 1.4509 - learning_rate: 1.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1172s\u001b[0m 2s/step - accuracy: 0.5575 - loss: 1.5316 - val_accuracy: 0.5903 - val_loss: 1.4211 - learning_rate: 1.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1179s\u001b[0m 2s/step - accuracy: 0.5733 - loss: 1.4895 - val_accuracy: 0.5669 - val_loss: 1.4823 - learning_rate: 1.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1181s\u001b[0m 2s/step - accuracy: 0.5860 - loss: 1.4572 - val_accuracy: 0.5775 - val_loss: 1.4525 - learning_rate: 1.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1176s\u001b[0m 2s/step - accuracy: 0.5941 - loss: 1.4323 - val_accuracy: 0.5817 - val_loss: 1.4419 - learning_rate: 1.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1178s\u001b[0m 2s/step - accuracy: 0.5965 - loss: 1.4081 - val_accuracy: 0.6494 - val_loss: 1.2481 - learning_rate: 5.0000e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1194s\u001b[0m 2s/step - accuracy: 0.5950 - loss: 1.4152 - val_accuracy: 0.6318 - val_loss: 1.3028 - learning_rate: 5.0000e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1177s\u001b[0m 2s/step - accuracy: 0.6079 - loss: 1.3780 - val_accuracy: 0.6396 - val_loss: 1.2894 - learning_rate: 5.0000e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1194s\u001b[0m 2s/step - accuracy: 0.6150 - loss: 1.3769 - val_accuracy: 0.6619 - val_loss: 1.2530 - learning_rate: 5.0000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1180s\u001b[0m 2s/step - accuracy: 0.6121 - loss: 1.3745 - val_accuracy: 0.6458 - val_loss: 1.2717 - learning_rate: 2.5000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1178s\u001b[0m 2s/step - accuracy: 0.6221 - loss: 1.3382 - val_accuracy: 0.6603 - val_loss: 1.2209 - learning_rate: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    {'img_input': X_train_img, 'meta_input': X_train_meta},\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {'img_input': X_val_img, 'meta_input': X_val_meta},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6bf66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model for fine-tuning\n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b59c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1375s\u001b[0m 3s/step - accuracy: 0.6188 - loss: 1.3292 - val_accuracy: 0.6062 - val_loss: 1.3845 - learning_rate: 1.0000e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1332s\u001b[0m 3s/step - accuracy: 0.6238 - loss: 1.3349 - val_accuracy: 0.5513 - val_loss: 1.5215 - learning_rate: 1.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 3s/step - accuracy: 0.6400 - loss: 1.2830 - val_accuracy: 0.5619 - val_loss: 1.4953 - learning_rate: 1.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 3s/step - accuracy: 0.6491 - loss: 1.2770 - val_accuracy: 0.6407 - val_loss: 1.2622 - learning_rate: 1.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1280s\u001b[0m 3s/step - accuracy: 0.6462 - loss: 1.2693 - val_accuracy: 0.6321 - val_loss: 1.3010 - learning_rate: 1.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1292s\u001b[0m 3s/step - accuracy: 0.6572 - loss: 1.2352 - val_accuracy: 0.5633 - val_loss: 1.5000 - learning_rate: 1.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1278s\u001b[0m 3s/step - accuracy: 0.6678 - loss: 1.2288 - val_accuracy: 0.6151 - val_loss: 1.3606 - learning_rate: 1.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1371s\u001b[0m 3s/step - accuracy: 0.6804 - loss: 1.1890 - val_accuracy: 0.6661 - val_loss: 1.2042 - learning_rate: 5.0000e-06\n",
      "Epoch 29/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1341s\u001b[0m 3s/step - accuracy: 0.6703 - loss: 1.1870 - val_accuracy: 0.6580 - val_loss: 1.2209 - learning_rate: 5.0000e-06\n",
      "Epoch 30/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1309s\u001b[0m 3s/step - accuracy: 0.6825 - loss: 1.1753 - val_accuracy: 0.6745 - val_loss: 1.1706 - learning_rate: 5.0000e-06\n",
      "Epoch 31/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1344s\u001b[0m 3s/step - accuracy: 0.6767 - loss: 1.1800 - val_accuracy: 0.6630 - val_loss: 1.2167 - learning_rate: 5.0000e-06\n",
      "Epoch 32/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1333s\u001b[0m 3s/step - accuracy: 0.6857 - loss: 1.1717 - val_accuracy: 0.6458 - val_loss: 1.2603 - learning_rate: 5.0000e-06\n",
      "Epoch 33/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1344s\u001b[0m 3s/step - accuracy: 0.6861 - loss: 1.1547 - val_accuracy: 0.6628 - val_loss: 1.1985 - learning_rate: 5.0000e-06\n",
      "Epoch 34/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1366s\u001b[0m 3s/step - accuracy: 0.6918 - loss: 1.1452 - val_accuracy: 0.6792 - val_loss: 1.1749 - learning_rate: 2.5000e-06\n",
      "Epoch 35/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1360s\u001b[0m 3s/step - accuracy: 0.6932 - loss: 1.1497 - val_accuracy: 0.6745 - val_loss: 1.1737 - learning_rate: 2.5000e-06\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training the model with fine-tuning\n",
    "history_fine = model.fit(\n",
    "    {'img_input': X_train_img, 'meta_input': X_train_meta}, \n",
    "    y_train,                                                \n",
    "    validation_data=(\n",
    "        {'img_input': X_val_img, 'meta_input': X_val_meta},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1905eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n"
     ]
    }
   ],
   "source": [
    "print(history_fine.epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "519d3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1276s\u001b[0m 2s/step - accuracy: 0.6783 - loss: 1.1827 - val_accuracy: 0.6494 - val_loss: 1.2510 - learning_rate: 2.5000e-06\n",
      "Epoch 17/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1345s\u001b[0m 3s/step - accuracy: 0.6911 - loss: 1.1561 - val_accuracy: 0.6689 - val_loss: 1.1907 - learning_rate: 2.5000e-06\n",
      "Epoch 18/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2478s\u001b[0m 5s/step - accuracy: 0.6881 - loss: 1.1555 - val_accuracy: 0.6683 - val_loss: 1.2046 - learning_rate: 2.5000e-06\n",
      "Epoch 19/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1790s\u001b[0m 4s/step - accuracy: 0.6960 - loss: 1.1444 - val_accuracy: 0.6803 - val_loss: 1.1688 - learning_rate: 2.5000e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2277s\u001b[0m 4s/step - accuracy: 0.6904 - loss: 1.1454 - val_accuracy: 0.6828 - val_loss: 1.1628 - learning_rate: 2.5000e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1871s\u001b[0m 4s/step - accuracy: 0.6965 - loss: 1.1491 - val_accuracy: 0.6642 - val_loss: 1.2176 - learning_rate: 2.5000e-06\n",
      "Epoch 22/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1394s\u001b[0m 3s/step - accuracy: 0.6904 - loss: 1.1360 - val_accuracy: 0.6750 - val_loss: 1.1824 - learning_rate: 2.5000e-06\n",
      "Epoch 23/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1359s\u001b[0m 3s/step - accuracy: 0.6903 - loss: 1.1431 - val_accuracy: 0.6792 - val_loss: 1.1716 - learning_rate: 2.5000e-06\n",
      "Epoch 24/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 3s/step - accuracy: 0.6923 - loss: 1.1586 - val_accuracy: 0.6842 - val_loss: 1.1388 - learning_rate: 1.2500e-06\n",
      "Epoch 25/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1425s\u001b[0m 3s/step - accuracy: 0.6975 - loss: 1.1276 - val_accuracy: 0.6800 - val_loss: 1.1716 - learning_rate: 1.2500e-06\n",
      "Epoch 26/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1343s\u001b[0m 3s/step - accuracy: 0.6987 - loss: 1.1325 - val_accuracy: 0.6834 - val_loss: 1.1605 - learning_rate: 1.2500e-06\n",
      "Epoch 27/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1347s\u001b[0m 3s/step - accuracy: 0.6963 - loss: 1.1228 - val_accuracy: 0.6873 - val_loss: 1.1405 - learning_rate: 1.2500e-06\n",
      "Epoch 28/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1389s\u001b[0m 3s/step - accuracy: 0.6959 - loss: 1.1246 - val_accuracy: 0.6828 - val_loss: 1.1440 - learning_rate: 1.0000e-06\n",
      "Epoch 29/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1655s\u001b[0m 3s/step - accuracy: 0.6978 - loss: 1.1306 - val_accuracy: 0.6851 - val_loss: 1.1419 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Continue training the model with fine-tuning\n",
    "history_fine1 = model.fit(\n",
    "    {'img_input': X_train_img, 'meta_input': X_train_meta}, \n",
    "    y_train,                                                \n",
    "    validation_data=(\n",
    "        {'img_input': X_val_img, 'meta_input': X_val_meta},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=len(history_fine.epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aefbab47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1838s\u001b[0m 4s/step - accuracy: 0.7008 - loss: 1.1364 - val_accuracy: 0.6842 - val_loss: 1.1386 - learning_rate: 1.0000e-06\n",
      "Epoch 17/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1383s\u001b[0m 3s/step - accuracy: 0.6948 - loss: 1.1345 - val_accuracy: 0.6814 - val_loss: 1.1546 - learning_rate: 1.0000e-06\n",
      "Epoch 18/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2821s\u001b[0m 6s/step - accuracy: 0.7079 - loss: 1.1169 - val_accuracy: 0.6851 - val_loss: 1.1497 - learning_rate: 1.0000e-06\n",
      "Epoch 19/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1688s\u001b[0m 3s/step - accuracy: 0.7018 - loss: 1.1349 - val_accuracy: 0.6828 - val_loss: 1.1446 - learning_rate: 1.0000e-06\n",
      "Epoch 20/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1429s\u001b[0m 3s/step - accuracy: 0.7008 - loss: 1.1161 - val_accuracy: 0.6767 - val_loss: 1.1751 - learning_rate: 1.0000e-06\n",
      "Epoch 21/50\n",
      "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1392s\u001b[0m 3s/step - accuracy: 0.6956 - loss: 1.1370 - val_accuracy: 0.6775 - val_loss: 1.1799 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "history_fine2 = model.fit(\n",
    "    {'img_input': X_train_img, 'meta_input': X_train_meta}, \n",
    "    y_train,                                                \n",
    "    validation_data=(\n",
    "        {'img_input': X_val_img, 'meta_input': X_val_meta},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    initial_epoch=len(history_fine.epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb4a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
